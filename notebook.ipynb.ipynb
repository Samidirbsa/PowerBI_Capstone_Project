{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CAPSTONE DEFENSE PROJECT**\n",
    "# **POWER BI** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Business Understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Project Description/Background\n",
    "\n",
    "The objective of this Power BI Dashboard project is to transform the raw transactional data collected by the client in the year 2019 into actionable insights. By leveraging business intelligence tools, we aim to empower our client to make informed decisions to drive sales and enhance operational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "\n",
    "The primary objective of this Power BI Dashboard project is to leverage the collected transactional data from the year 2019 to provide actionable insights for our client. Specifically, the objective encompasses the following key points:\n",
    "\n",
    "1. Revenue Analysis: Determine the total revenue generated throughout the year 2019, providing a clear understanding of the financial performance over the specified period.\n",
    "\n",
    "2. Seasonality Assessment: Identify any recurring patterns or seasonality in sales data to facilitate better resource allocation, inventory management, and marketing strategies.\n",
    "\n",
    "3. Product Performance Evaluation: Analyze sales data to identify the best-selling and worst-selling products, enabling the optimization of product offerings and inventory management practices.\n",
    "\n",
    "4. Sales Trend Analysis: Compare sales performance across different time periods (months or weeks) to identify trends, fluctuations, and potential areas for improvement or expansion.\n",
    "\n",
    "5. Geographical Insights: Determine the distribution of product deliveries across various cities to enable targeted marketing efforts and optimize logistics operations.\n",
    "\n",
    "6. Product Category Comparison: Compare revenue generated and quantities ordered across different product categories, providing insights into the performance of various product lines and guiding future product development strategies.\n",
    "\n",
    "7. Additional Details Integration: Incorporate additional details from the data findings to provide a comprehensive understanding of business performance, including the classification of products into high-level and basic categories based on unit prices.\n",
    "\n",
    "By achieving these objectives, the Power BI Dashboard will empower our client to make data-driven decisions, enhance sales strategies, optimize operations, and drive overall business growth and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Features\n",
    "\n",
    "The dataset provided contains the following fields:\n",
    "\n",
    "1. Order ID: Unique identifier for each order placed.\n",
    "2. Product: Name or description of the product purchased.\n",
    "4. Quantity Ordered: The number of units of the product ordered in each transaction.\n",
    "5. Price Each: The unit price of the product.\n",
    "6. Order Date: Date and time when the order was placed.\n",
    "7. Purchase Address: Address where the purchase was made or where the products were delivered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  **Null Hypothesis:** There is no difference in revenue generated between different product categories.\n",
    "  \n",
    "  **Alternative Hypothesis:** Certain product categories generate significantly more revenue compared to others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Analytical Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How much money did we make this year? \n",
    "\n",
    "2. Can we identify any seasonality in the  sales? \n",
    "\n",
    "3. What are our best and worst-selling products? \n",
    "\n",
    "4. How do sales compare to previous months or weeks? \n",
    "\n",
    "5. Which cities are our products delivered to most? \n",
    "\n",
    "6. How do product categories compare in revenue generated and quantities  ordered? \n",
    "\n",
    "7. What are the best and worst-selling products?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis and Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation packages\n",
    "import pyodbc\n",
    "from scipy.stats import f_oneway\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import dotenv_values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Data visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data Sales January, February, March, April, May, June 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data from SQL Server for July - December 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file into a dictionary\n",
    "environment_variables = dotenv_values('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values for the credentials you set in the '.env' file\n",
    "server = environment_variables.get(\"SERVER\")\n",
    "database = environment_variables.get(\"DATABASE\")\n",
    "username = environment_variables.get(\"UID\")\n",
    "password = environment_variables.get(\"PWD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define connection string with appropriate parameters\n",
    "\n",
    "connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the database using the provided connection string.\n",
    "connection= pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_January_2019 = pd.read_csv('Power BI Capstione Data - (Jan -May)/Sales_January_2019.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_sales_January_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150502</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>02/18/19 01:35</td>\n",
       "      <td>866 Spruce St, Portland, ME 04101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150503</td>\n",
       "      <td>AA Batteries (4-pack)</td>\n",
       "      <td>1</td>\n",
       "      <td>3.84</td>\n",
       "      <td>02/13/19 07:24</td>\n",
       "      <td>18 13th St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150504</td>\n",
       "      <td>27in 4K Gaming Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>389.99</td>\n",
       "      <td>02/18/19 09:46</td>\n",
       "      <td>52 6th St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150505</td>\n",
       "      <td>Lightning Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>14.95</td>\n",
       "      <td>02/02/19 16:47</td>\n",
       "      <td>129 Cherry St, Atlanta, GA 30301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150506</td>\n",
       "      <td>AA Batteries (4-pack)</td>\n",
       "      <td>2</td>\n",
       "      <td>3.84</td>\n",
       "      <td>02/28/19 20:32</td>\n",
       "      <td>548 Lincoln St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order ID                   Product Quantity Ordered Price Each  \\\n",
       "0   150502                    iPhone                1        700   \n",
       "1   150503     AA Batteries (4-pack)                1       3.84   \n",
       "2   150504    27in 4K Gaming Monitor                1     389.99   \n",
       "3   150505  Lightning Charging Cable                1      14.95   \n",
       "4   150506     AA Batteries (4-pack)                2       3.84   \n",
       "\n",
       "       Order Date                     Purchase Address  \n",
       "0  02/18/19 01:35    866 Spruce St, Portland, ME 04101  \n",
       "1  02/13/19 07:24  18 13th St, San Francisco, CA 94016  \n",
       "2  02/18/19 09:46   52 6th St, New York City, NY 10001  \n",
       "3  02/02/19 16:47     129 Cherry St, Atlanta, GA 30301  \n",
       "4  02/28/19 20:32    548 Lincoln St, Seattle, WA 98101  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_February_2019 = pd.read_csv('Power BI Capstione Data - (Jan -May)/Sales_February_2019.csv')\n",
    "# Display the first few rows of the DataFrame\n",
    "df_sales_February_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>162009</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>03/28/19 20:59</td>\n",
       "      <td>942 Church St, Austin, TX 73301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162009</td>\n",
       "      <td>Lightning Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>14.95</td>\n",
       "      <td>03/28/19 20:59</td>\n",
       "      <td>942 Church St, Austin, TX 73301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162009</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>2</td>\n",
       "      <td>11.99</td>\n",
       "      <td>03/28/19 20:59</td>\n",
       "      <td>942 Church St, Austin, TX 73301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>162010</td>\n",
       "      <td>Bose SoundSport Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>99.99</td>\n",
       "      <td>03/17/19 05:39</td>\n",
       "      <td>261 10th St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>162011</td>\n",
       "      <td>34in Ultrawide Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>379.99</td>\n",
       "      <td>03/10/19 00:01</td>\n",
       "      <td>764 13th St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order ID                     Product Quantity Ordered Price Each  \\\n",
       "0   162009                      iPhone                1        700   \n",
       "1   162009    Lightning Charging Cable                1      14.95   \n",
       "2   162009            Wired Headphones                2      11.99   \n",
       "3   162010  Bose SoundSport Headphones                1      99.99   \n",
       "4   162011      34in Ultrawide Monitor                1     379.99   \n",
       "\n",
       "       Order Date                      Purchase Address  \n",
       "0  03/28/19 20:59       942 Church St, Austin, TX 73301  \n",
       "1  03/28/19 20:59       942 Church St, Austin, TX 73301  \n",
       "2  03/28/19 20:59       942 Church St, Austin, TX 73301  \n",
       "3  03/17/19 05:39  261 10th St, San Francisco, CA 94016  \n",
       "4  03/10/19 00:01  764 13th St, San Francisco, CA 94016  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_March_2019 = pd.read_csv('Power BI Capstione Data - (Jan -May)/Sales_March_2019.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_sales_March_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176558</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>2</td>\n",
       "      <td>11.95</td>\n",
       "      <td>04/19/19 08:46</td>\n",
       "      <td>917 1st St, Dallas, TX 75001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>176559</td>\n",
       "      <td>Bose SoundSport Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>99.99</td>\n",
       "      <td>04/07/19 22:30</td>\n",
       "      <td>682 Chestnut St, Boston, MA 02215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>176560</td>\n",
       "      <td>Google Phone</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>04/12/19 14:38</td>\n",
       "      <td>669 Spruce St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176560</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "      <td>04/12/19 14:38</td>\n",
       "      <td>669 Spruce St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order ID                     Product Quantity Ordered Price Each  \\\n",
       "0   176558        USB-C Charging Cable                2      11.95   \n",
       "1      NaN                         NaN              NaN        NaN   \n",
       "2   176559  Bose SoundSport Headphones                1      99.99   \n",
       "3   176560                Google Phone                1        600   \n",
       "4   176560            Wired Headphones                1      11.99   \n",
       "\n",
       "       Order Date                      Purchase Address  \n",
       "0  04/19/19 08:46          917 1st St, Dallas, TX 75001  \n",
       "1             NaN                                   NaN  \n",
       "2  04/07/19 22:30     682 Chestnut St, Boston, MA 02215  \n",
       "3  04/12/19 14:38  669 Spruce St, Los Angeles, CA 90001  \n",
       "4  04/12/19 14:38  669 Spruce St, Los Angeles, CA 90001  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_April_2019 = pd.read_csv('Power BI Capstione Data - (Jan -May)/Sales_April_2019.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_sales_April_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194095</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "      <td>05/16/19 17:14</td>\n",
       "      <td>669 2nd St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194096</td>\n",
       "      <td>AA Batteries (4-pack)</td>\n",
       "      <td>1</td>\n",
       "      <td>3.84</td>\n",
       "      <td>05/19/19 14:43</td>\n",
       "      <td>844 Walnut St, Dallas, TX 75001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194097</td>\n",
       "      <td>27in FHD Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>149.99</td>\n",
       "      <td>05/24/19 11:36</td>\n",
       "      <td>164 Madison St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194098</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "      <td>05/02/19 20:40</td>\n",
       "      <td>622 Meadow St, Dallas, TX 75001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>194099</td>\n",
       "      <td>AAA Batteries (4-pack)</td>\n",
       "      <td>2</td>\n",
       "      <td>2.99</td>\n",
       "      <td>05/11/19 22:55</td>\n",
       "      <td>17 Church St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order ID                 Product Quantity Ordered Price Each  \\\n",
       "0   194095        Wired Headphones                1      11.99   \n",
       "1   194096   AA Batteries (4-pack)                1       3.84   \n",
       "2   194097        27in FHD Monitor                1     149.99   \n",
       "3   194098        Wired Headphones                1      11.99   \n",
       "4   194099  AAA Batteries (4-pack)                2       2.99   \n",
       "\n",
       "       Order Date                         Purchase Address  \n",
       "0  05/16/19 17:14      669 2nd St, New York City, NY 10001  \n",
       "1  05/19/19 14:43          844 Walnut St, Dallas, TX 75001  \n",
       "2  05/24/19 11:36  164 Madison St, New York City, NY 10001  \n",
       "3  05/02/19 20:40          622 Meadow St, Dallas, TX 75001  \n",
       "4  05/11/19 22:55          17 Church St, Seattle, WA 98101  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_May_2019 = pd.read_csv('Power BI Capstione Data - (Jan -May)/Sales_May_2019.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_sales_May_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>209921</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>11.95</td>\n",
       "      <td>06/23/19 19:34</td>\n",
       "      <td>950 Walnut St, Portland, ME 04101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>209922</td>\n",
       "      <td>Macbook Pro Laptop</td>\n",
       "      <td>1</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>06/30/19 10:05</td>\n",
       "      <td>80 4th St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>209923</td>\n",
       "      <td>ThinkPad Laptop</td>\n",
       "      <td>1</td>\n",
       "      <td>999.99</td>\n",
       "      <td>06/24/19 20:18</td>\n",
       "      <td>402 Jackson St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>209924</td>\n",
       "      <td>27in FHD Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>149.99</td>\n",
       "      <td>06/05/19 10:21</td>\n",
       "      <td>560 10th St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>209925</td>\n",
       "      <td>Bose SoundSport Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>99.99</td>\n",
       "      <td>06/25/19 18:58</td>\n",
       "      <td>545 2nd St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Order ID                     Product Quantity Ordered Price Each  \\\n",
       "0   209921        USB-C Charging Cable                1      11.95   \n",
       "1   209922          Macbook Pro Laptop                1     1700.0   \n",
       "2   209923             ThinkPad Laptop                1     999.99   \n",
       "3   209924            27in FHD Monitor                1     149.99   \n",
       "4   209925  Bose SoundSport Headphones                1      99.99   \n",
       "\n",
       "       Order Date                       Purchase Address  \n",
       "0  06/23/19 19:34      950 Walnut St, Portland, ME 04101  \n",
       "1  06/30/19 10:05     80 4th St, San Francisco, CA 94016  \n",
       "2  06/24/19 20:18  402 Jackson St, Los Angeles, CA 90001  \n",
       "3  06/05/19 10:21         560 10th St, Seattle, WA 98101  \n",
       "4  06/25/19 18:58    545 2nd St, San Francisco, CA 94016  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_June_2019 = pd.read_csv('Power BI Capstione Data - (Jan -May)/Sales_June_2019.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_sales_June_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sami\\AppData\\Local\\Temp\\ipykernel_3344\\3596020920.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dap_july = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order_ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity_Ordered</th>\n",
       "      <th>Price_Each</th>\n",
       "      <th>Order_Date</th>\n",
       "      <th>Purchase_Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>222910.0</td>\n",
       "      <td>Apple Airpods Headphones</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.00</td>\n",
       "      <td>2026-07-19 16:51:00.0000000</td>\n",
       "      <td>389 South St, Atlanta, GA 30301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>222911.0</td>\n",
       "      <td>Flatscreen TV</td>\n",
       "      <td>1.0</td>\n",
       "      <td>300.00</td>\n",
       "      <td>2005-07-19 08:55:00.0000000</td>\n",
       "      <td>590 4th St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222912.0</td>\n",
       "      <td>AA Batteries (4-pack)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2029-07-19 12:41:00.0000000</td>\n",
       "      <td>861 Hill St, Atlanta, GA 30301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>222913.0</td>\n",
       "      <td>AA Batteries (4-pack)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2028-07-19 10:15:00.0000000</td>\n",
       "      <td>190 Ridge St, Atlanta, GA 30301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>222914.0</td>\n",
       "      <td>AAA Batteries (4-pack)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2031-07-19 02:13:00.0000000</td>\n",
       "      <td>824 Forest St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order_ID                   Product  Quantity_Ordered  Price_Each  \\\n",
       "0  222910.0  Apple Airpods Headphones               1.0      150.00   \n",
       "1  222911.0             Flatscreen TV               1.0      300.00   \n",
       "2  222912.0     AA Batteries (4-pack)               1.0        3.84   \n",
       "3  222913.0     AA Batteries (4-pack)               1.0        3.84   \n",
       "4  222914.0    AAA Batteries (4-pack)               5.0        2.99   \n",
       "\n",
       "                    Order_Date                  Purchase_Address  \n",
       "0  2026-07-19 16:51:00.0000000   389 South St, Atlanta, GA 30301  \n",
       "1  2005-07-19 08:55:00.0000000     590 4th St, Seattle, WA 98101  \n",
       "2  2029-07-19 12:41:00.0000000    861 Hill St, Atlanta, GA 30301  \n",
       "3  2028-07-19 10:15:00.0000000   190 Ridge St, Atlanta, GA 30301  \n",
       "4  2031-07-19 02:13:00.0000000  824 Forest St, Seattle, WA 98101  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL query to fetch data from 'dbo.LP1_startup_funding2020' table\n",
    "query = \"Select * from dbo.Sales_July_2019\"\n",
    "\n",
    "# Read data from the SQL query result\n",
    "dap_july = pd.read_sql(query, connection)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "dap_july.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sami\\AppData\\Local\\Temp\\ipykernel_3344\\949500391.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dap_august = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order_ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity_Ordered</th>\n",
       "      <th>Price_Each</th>\n",
       "      <th>Order_Date</th>\n",
       "      <th>Purchase_Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>236670.0</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.990000</td>\n",
       "      <td>2031-08-19 22:21:00.0000000</td>\n",
       "      <td>359 Spruce St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>236671.0</td>\n",
       "      <td>Bose SoundSport Headphones</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.989998</td>\n",
       "      <td>2015-08-19 15:11:00.0000000</td>\n",
       "      <td>492 Ridge St, Dallas, TX 75001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>236672.0</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>1.0</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>2006-08-19 14:40:00.0000000</td>\n",
       "      <td>149 7th St, Portland, OR 97035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>236673.0</td>\n",
       "      <td>AA Batteries (4-pack)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.840000</td>\n",
       "      <td>2029-08-19 20:59:00.0000000</td>\n",
       "      <td>631 2nd St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>236674.0</td>\n",
       "      <td>AA Batteries (4-pack)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.840000</td>\n",
       "      <td>2015-08-19 19:53:00.0000000</td>\n",
       "      <td>736 14th St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order_ID                     Product  Quantity_Ordered  Price_Each  \\\n",
       "0  236670.0            Wired Headphones               2.0   11.990000   \n",
       "1  236671.0  Bose SoundSport Headphones               1.0   99.989998   \n",
       "2  236672.0                      iPhone               1.0  700.000000   \n",
       "3  236673.0       AA Batteries (4-pack)               2.0    3.840000   \n",
       "4  236674.0       AA Batteries (4-pack)               2.0    3.840000   \n",
       "\n",
       "                    Order_Date                      Purchase_Address  \n",
       "0  2031-08-19 22:21:00.0000000      359 Spruce St, Seattle, WA 98101  \n",
       "1  2015-08-19 15:11:00.0000000        492 Ridge St, Dallas, TX 75001  \n",
       "2  2006-08-19 14:40:00.0000000        149 7th St, Portland, OR 97035  \n",
       "3  2029-08-19 20:59:00.0000000     631 2nd St, Los Angeles, CA 90001  \n",
       "4  2015-08-19 19:53:00.0000000  736 14th St, New York City, NY 10001  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Select * from dbo.Sales_August_2019\"\n",
    "\n",
    "dap_august = pd.read_sql(query, connection)\n",
    "\n",
    "dap_august.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sami\\AppData\\Local\\Temp\\ipykernel_3344\\4226239043.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dap_sept = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order_ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity_Ordered</th>\n",
       "      <th>Price_Each</th>\n",
       "      <th>Order_Date</th>\n",
       "      <th>Purchase_Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>248151.0</td>\n",
       "      <td>AA Batteries (4-pack)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.840000</td>\n",
       "      <td>2017-09-19 14:44:00.0000000</td>\n",
       "      <td>380 North St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>248152.0</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.950000</td>\n",
       "      <td>2029-09-19 10:19:00.0000000</td>\n",
       "      <td>511 8th St, Austin, TX 73301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>248153.0</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.950000</td>\n",
       "      <td>2016-09-19 17:48:00.0000000</td>\n",
       "      <td>151 Johnson St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>248154.0</td>\n",
       "      <td>27in FHD Monitor</td>\n",
       "      <td>1.0</td>\n",
       "      <td>149.990005</td>\n",
       "      <td>2027-09-19 07:52:00.0000000</td>\n",
       "      <td>355 Hickory St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>248155.0</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.950000</td>\n",
       "      <td>2001-09-19 19:03:00.0000000</td>\n",
       "      <td>125 5th St, Atlanta, GA 30301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order_ID                Product  Quantity_Ordered  Price_Each  \\\n",
       "0  248151.0  AA Batteries (4-pack)               4.0    3.840000   \n",
       "1  248152.0   USB-C Charging Cable               2.0   11.950000   \n",
       "2  248153.0   USB-C Charging Cable               1.0   11.950000   \n",
       "3  248154.0       27in FHD Monitor               1.0  149.990005   \n",
       "4  248155.0   USB-C Charging Cable               1.0   11.950000   \n",
       "\n",
       "                    Order_Date                       Purchase_Address  \n",
       "0  2017-09-19 14:44:00.0000000    380 North St, Los Angeles, CA 90001  \n",
       "1  2029-09-19 10:19:00.0000000           511 8th St, Austin, TX 73301  \n",
       "2  2016-09-19 17:48:00.0000000  151 Johnson St, Los Angeles, CA 90001  \n",
       "3  2027-09-19 07:52:00.0000000      355 Hickory St, Seattle, WA 98101  \n",
       "4  2001-09-19 19:03:00.0000000          125 5th St, Atlanta, GA 30301  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query= \"Select * from dbo.Sales_September_2019\"\n",
    "\n",
    "dap_sept = pd.read_sql(query, connection)\n",
    "\n",
    "dap_sept.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sami\\AppData\\Local\\Temp\\ipykernel_3344\\2242588360.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dap_oct = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order_ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity_Ordered</th>\n",
       "      <th>Price_Each</th>\n",
       "      <th>Order_Date</th>\n",
       "      <th>Purchase_Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>259358.0</td>\n",
       "      <td>34in Ultrawide Monitor</td>\n",
       "      <td>1.0</td>\n",
       "      <td>379.989990</td>\n",
       "      <td>2028-10-19 10:56:00.0000000</td>\n",
       "      <td>609 Cherry St, Dallas, TX 75001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>259359.0</td>\n",
       "      <td>27in 4K Gaming Monitor</td>\n",
       "      <td>1.0</td>\n",
       "      <td>389.989990</td>\n",
       "      <td>2028-10-19 17:26:00.0000000</td>\n",
       "      <td>225 5th St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>259360.0</td>\n",
       "      <td>AAA Batteries (4-pack)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.990000</td>\n",
       "      <td>2024-10-19 17:20:00.0000000</td>\n",
       "      <td>967 12th St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>259361.0</td>\n",
       "      <td>27in FHD Monitor</td>\n",
       "      <td>1.0</td>\n",
       "      <td>149.990005</td>\n",
       "      <td>2014-10-19 22:26:00.0000000</td>\n",
       "      <td>628 Jefferson St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>259362.0</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.990000</td>\n",
       "      <td>2007-10-19 16:10:00.0000000</td>\n",
       "      <td>534 14th St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order_ID                 Product  Quantity_Ordered  Price_Each  \\\n",
       "0  259358.0  34in Ultrawide Monitor               1.0  379.989990   \n",
       "1  259359.0  27in 4K Gaming Monitor               1.0  389.989990   \n",
       "2  259360.0  AAA Batteries (4-pack)               2.0    2.990000   \n",
       "3  259361.0        27in FHD Monitor               1.0  149.990005   \n",
       "4  259362.0        Wired Headphones               1.0   11.990000   \n",
       "\n",
       "                    Order_Date                           Purchase_Address  \n",
       "0  2028-10-19 10:56:00.0000000            609 Cherry St, Dallas, TX 75001  \n",
       "1  2028-10-19 17:26:00.0000000          225 5th St, Los Angeles, CA 90001  \n",
       "2  2024-10-19 17:20:00.0000000       967 12th St, New York City, NY 10001  \n",
       "3  2014-10-19 22:26:00.0000000  628 Jefferson St, New York City, NY 10001  \n",
       "4  2007-10-19 16:10:00.0000000         534 14th St, Los Angeles, CA 90001  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query= \"Select * from dbo.Sales_October_2019\"\n",
    "\n",
    "dap_oct = pd.read_sql(query, connection)\n",
    "\n",
    "dap_oct.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sami\\AppData\\Local\\Temp\\ipykernel_3344\\126429497.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dap_nov = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order_ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity_Ordered</th>\n",
       "      <th>Price_Each</th>\n",
       "      <th>Order_Date</th>\n",
       "      <th>Purchase_Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>278797.0</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.990000</td>\n",
       "      <td>2021-11-19 09:54:00.0000000</td>\n",
       "      <td>46 Park St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>278798.0</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.950000</td>\n",
       "      <td>2017-11-19 10:03:00.0000000</td>\n",
       "      <td>962 Hickory St, Austin, TX 73301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>278799.0</td>\n",
       "      <td>Apple Airpods Headphones</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>2019-11-19 14:56:00.0000000</td>\n",
       "      <td>464 Cherry St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>278800.0</td>\n",
       "      <td>27in FHD Monitor</td>\n",
       "      <td>1.0</td>\n",
       "      <td>149.990005</td>\n",
       "      <td>2025-11-19 22:24:00.0000000</td>\n",
       "      <td>649 10th St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>278801.0</td>\n",
       "      <td>Bose SoundSport Headphones</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.989998</td>\n",
       "      <td>2009-11-19 13:56:00.0000000</td>\n",
       "      <td>522 Hill St, Boston, MA 02215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order_ID                     Product  Quantity_Ordered  Price_Each  \\\n",
       "0  278797.0            Wired Headphones               1.0   11.990000   \n",
       "1  278798.0        USB-C Charging Cable               2.0   11.950000   \n",
       "2  278799.0    Apple Airpods Headphones               1.0  150.000000   \n",
       "3  278800.0            27in FHD Monitor               1.0  149.990005   \n",
       "4  278801.0  Bose SoundSport Headphones               1.0   99.989998   \n",
       "\n",
       "                    Order_Date                      Purchase_Address  \n",
       "0  2021-11-19 09:54:00.0000000   46 Park St, New York City, NY 10001  \n",
       "1  2017-11-19 10:03:00.0000000      962 Hickory St, Austin, TX 73301  \n",
       "2  2019-11-19 14:56:00.0000000  464 Cherry St, Los Angeles, CA 90001  \n",
       "3  2025-11-19 22:24:00.0000000        649 10th St, Seattle, WA 98101  \n",
       "4  2009-11-19 13:56:00.0000000         522 Hill St, Boston, MA 02215  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "query = \"Select * from dbo.Sales_November_2019\"\n",
    "\n",
    "dap_nov = pd.read_sql(query, connection)\n",
    "\n",
    "dap_nov.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Select * from dbo.Sales_December_2019\"\n",
    "\n",
    "dap_dec = pd.read_sql(query, connection)\n",
    "\n",
    "dap_dec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "JANUARY 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_January_2019.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_January_2019.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_January_2019.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There are 50 duplicate rows in the DataFrame df_sales_January_2019. Duplicate rows may indicate errors in data collection or entry and may need to be investigated further to ensure data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "df_sales_January_2019.isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "df_sales_January_2019.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame df_sales_January_2019 contains sales data for January 2019. Here's a summary of its descriptive statistics:\n",
    "\n",
    "1. Order ID: There are 9697 entries. However, there are 9269 unique Order IDs, indicating some duplication or inconsistency. The most frequent entry is \"Order ID\" with a frequency of 16, suggesting potential header repetition or missing values.\n",
    "2. Product: There are 9697 entries and 20 unique products. The most common product is the \"USB-C Charging Cable\" with a frequency of 1171, indicating its popularity among customers.\n",
    "3. Quantity Ordered: There are 9697 entries, and the quantity ordered ranges from 1 to 8 units. The most common quantity ordered is 1, occurring 8795 times.\n",
    "4. Price Each: There are 9697 entries, and the price of each item ranges from $2.99 to $1700. The most common price is $11.95, occurring 1171 times.\n",
    "5. Order Date: There are 9697 entries, and 8077 unique order dates. The most frequent entry is \"Order Date\" with a frequency of 16, indicating potential inconsistency or repetition.\n",
    "6. Purchase Address: There are 9697 entries, and 9161 unique purchase addresses. The most frequent entry is \"Purchase Address\" with a frequency of 16, indicating potential inconsistency or repetition.\n",
    "\n",
    "Overall, the summary suggests that there may be inconsistencies and missing values in the dataset, particularly in the \"Order ID\" and \"Order Date\" columns. Cleaning and preprocessing may be necessary before further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = df_sales_January_2019.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEBRUARY 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_February_2019.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_February_2019.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_February_2019.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DataFrame for February 2019 sales (df_sales_February_2019), there are 66 duplicate rows. These duplicates might indicate errors in data entry or repeated transactions, and they should be reviewed and potentially removed to ensure the accuracy of the dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "df_sales_February_2019.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset for February 2019 sales (df_sales_February_2019), each column contains 32 missing values. This suggests potential data entry issues or gaps in the data collection process for that month. These missing values need to be addressed to ensure the integrity and completeness of the dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "df_sales_February_2019.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Order ID: There are 12004 entries. However, there are 11508 unique Order IDs, indicating some duplication or inconsistency. The most frequent entry is \"Order ID\" with a frequency of 18, suggesting potential header repetition or missing values.\n",
    "2. Product: There are 12004 entries and 20 unique products. The most common product is the \"USB-C Charging Cable\" with a frequency of 1514, indicating its popularity among customers.\n",
    "3. Quantity Ordered: There are 12004 entries, and the quantity ordered ranges from 1 to 8 units. The most common quantity ordered is 1, occurring 10863 times.\n",
    "4. Price Each: There are 12004 entries, and the price of each item ranges from $2.99 to $1700. The most common price is $11.95, occurring 1514 times.\n",
    "5. Order Date: There are 12004 entries, and 9627 unique order dates. The most frequent entry is \"Order Date\" with a frequency of 18, indicating potential inconsistency or repetition.\n",
    "6. Purchase Address: There are 12004 entries, and 11316 unique purchase addresses. The most frequent entry is \"Purchase Address\" with a frequency of 18, indicating potential inconsistency or repetition.\n",
    "Similarly to the previous summary, there may be inconsistencies and missing values in the dataset, particularly in the \"Order ID\" and \"Order Date\" columns. Cleaning and preprocessing may be necessary before further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = df_sales_February_2019.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MARCH 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_March_2019.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_March_2019.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_March_2019.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of duplicate rows in the DataFrame for March 2019 sales (df_sales_March_2019) is 95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "df_sales_March_2019.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DataFrame for March 2019 sales (df_sales_March_2019), there are 37 missing values in each column: Order ID, Product, Quantity Ordered, Price Each, Order Date, and Purchase Address. These missing values need to be handled appropriately before further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "df_sales_March_2019.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DataFrame for March 2019 sales (df_sales_March_2019), the descriptive statistics are as follows:\n",
    "\n",
    "1. Order ID: There are 15,189 entries. The number of unique order IDs is 14,550. The top value is \"Order ID,\" occurring 35 times.\n",
    "Product: There are 15,189 entries. The number of unique products is 20. The most frequent product is \"USB-C Charging Cable,\" occurring 1,770 times.\n",
    "2. Quantity Ordered: There are 15,189 entries. The number of unique quantities ordered is 8. The most frequent quantity ordered is 1, occurring 13,779 times.\n",
    "3. Price Each: There are 15,189 entries. The number of unique prices is 19. The top price is $11.95, occurring 1,770 times.\n",
    "Order Date: There are 15,189 entries. The number of unique order dates is 11,784. The top value is \"Order Date,\" occurring 35 times.\n",
    "4. Purchase Address: There are 15,189 entries. The number of unique purchase addresses is 14,247. The top value is \"Purchase Address,\" occurring 35 times.\n",
    "These statistics provide an overview of the data distribution and can help identify any anomalies or inconsistencies in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = df_sales_March_2019.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APRIL 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_April_2019.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_April_2019.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_April_2019.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are 114 duplicate rows in the April 2019 sales data DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "df_sales_April_2019.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The April 2019 sales data has 59 missing values in each of the columns: 'Order ID', 'Product', 'Quantity Ordered', 'Price Each', 'Order Date', and 'Purchase Address'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "df_sales_April_2019.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary statistics for the April 2019 sales data are as follows:\n",
    "\n",
    "1. Order ID: There are 18,324 entries. The number of unique order IDs is 17,538. The most frequent entry is \"Order ID\" with a frequency of 35.\n",
    "2. Product: There are 18,324 entries. There are 20 unique products. The most frequent product is \"Lightning Charging Cable\" with a frequency of 2,201.\n",
    "3. Quantity Ordered: There are 18,324 entries. There are 8 unique quantities ordered. The most frequent quantity ordered is 1, with a frequency of 16,558.\n",
    "4. Price Each: There are 18,324 entries. There are 19 unique prices. The most frequent price is $14.95, with a frequency of 2,201.\n",
    "5. Order Date: There are 18,324 entries. There are 13,584 unique order dates. The most frequent entry is \"Order Date\" with a frequency of 35.\n",
    "6. Purchase Address: There are 18,324 entries. There are 17,120 unique purchase addresses. The most frequent entry is \"Purchase Address\" with a frequency of 35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = df_sales_April_2019.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAY 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_May_2019.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_May_2019.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_May_2019.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the May 2019 sales data, there are 93 duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "df_sales_May_2019.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the May 2019 sales data, there are 48 missing values in each of the following columns: 'Order ID', 'Product', 'Quantity Ordered', 'Price Each', 'Order Date', and 'Purchase Address'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "df_sales_May_2019.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descriptive statistics of the May 2019 sales data indicate that:\n",
    "\n",
    "There are 16,587 records in the DataFrame.\n",
    "1. The 'Order ID' column has 15,827 unique values, indicating some duplication in this column.\n",
    "2. The 'Product' column has 20 unique values.\n",
    "3. The 'Quantity Ordered' column has 8 unique values.\n",
    "4. The 'Price Each' column has 22 unique values.\n",
    "5. The 'Order Date' column has 12,665 unique values, suggesting variability in the order dates.\n",
    "6. The 'Purchase Address' column has 15,461 unique values.\n",
    "The most frequent value in the 'Product' column is 'Lightning Charging Cable', with 1,932 occurrences.\n",
    "The most frequent value in the 'Quantity Ordered' column is '1', with 14,977 occurrences.\n",
    "The most frequent value in the 'Price Each' column is '14.95', with 1,932 occurrences.\n",
    "The most frequent value in the 'Order Date' column is 'Order Date', with 33 occurrences.\n",
    "The most frequent value in the 'Purchase Address' column is 'Purchase Address', with 33 occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = df_sales_May_2019.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JUNE 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_June_2019.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_June_2019.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_June_2019.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 83 duplicate rows in the June 2019 sales data DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "df_sales_June_2019.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the June 2019 sales data, there are 43 missing values in each of the following columns: Order ID, Product, Quantity Ordered, Price Each, Order Date, and Purchase Address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "df_sales_June_2019.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the June 2019 sales data:\n",
    "\n",
    "1. The Order ID column has 23 occurrences of the top value \"Order ID\".\n",
    "2. The Product column has 20 unique values.\n",
    "3. The Quantity Ordered column has 8 unique values.\n",
    "4. The Price Each column has 23 unique values.\n",
    "5. The Order Date column has 10,742 unique values.\n",
    "6. The Purchase Address column has 12,720 unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = df_sales_June_2019.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JULY 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_july.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_july.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_july.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In July, there are 96 duplicate rows in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_july.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In July, there are 80 missing values in the 'Order_ID', 'Quantity_Ordered', 'Price_Each', and 'Order_Date' columns. Additionally, there are 45 missing values in the 'Product' and 'Purchase_Address' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "dap_july.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In July, the sales data indicates the following:\n",
    "\n",
    "1. Order_ID: The orders range from 222910 to 236669, with an average of around 229788. The standard deviation is approximately 3970.66, suggesting a moderate dispersion around the mean.\n",
    "2. Quantity_Ordered: On average, each order consists of approximately 1.12 items, with a standard deviation of around 0.46. The minimum quantity ordered is 1, while the maximum is 9.\n",
    "3. Price_Each: The average price per item sold is approximately $184.15, with a considerable standard deviation of $332.95. Prices range from $2.99 to $1700.00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = dap_july.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUGUST 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_august.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_august.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_august.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 70 duplicate rows in the DataFrame dap_august."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_august.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame dap_august has missing values in the following columns:\n",
    "\n",
    "1. Order_ID: 54 missing values\n",
    "2. Product: 28 missing values\n",
    "3. Quantity_Ordered: 54 missing values\n",
    "4. Price_Each: 54 missing values\n",
    "5. Order_Date: 54 missing values\n",
    "6. Purchase_Address: 28 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "dap_august.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this summary:\n",
    "\n",
    "1. The Order_ID, Quantity_Ordered, and Price_Each columns have a count of 11,957 entries each.\n",
    "2. The mean quantity ordered is approximately 1.12, with a standard deviation of about 0.45.\n",
    "3. The mean price per item is around $186.53, with a standard deviation of approximately $332.30.\n",
    "4. The minimum Order_ID is 236,670, and the maximum is 248,150.\n",
    "5. The minimum quantity ordered is 1, and the maximum is 8.\n",
    "6. The prices range from $2.99 to $1700."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = dap_august.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEPTEMBER 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_sept.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_sept.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_sept.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 73 duplicate rows in the September dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_sept.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for September contains missing values in multiple columns:\n",
    "\n",
    "1. Order_ID: There are 57 missing values.\n",
    "2. Product: There are 40 missing values.\n",
    "3. Quantity_Ordered: There are 57 missing values.\n",
    "4. Price_Each: There are 57 missing values.\n",
    "5. Order_Date: There are 57 missing values.\n",
    "6. Purchase_Address: There are 40 missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "dap_sept.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 11,629 entries.\n",
    "\n",
    "For the Order_ID column:\n",
    "\n",
    "1. The minimum order ID is 248,151, and the maximum order ID is 259,357.\n",
    "The mean order ID is approximately 253,751.81, with a standard deviation of approximately 3,235.18.\n",
    "The median (50th percentile) order ID is 253,751.\n",
    "For the Quantity_Ordered column:\n",
    "\n",
    "2. The minimum quantity ordered is 1, and the maximum quantity ordered is 6.\n",
    "The mean quantity ordered is approximately 1.13, with a standard deviation of approximately 0.44.\n",
    "The median (50th percentile) quantity ordered is 1.\n",
    "For the Price_Each column:\n",
    "\n",
    "3. The minimum price is $2.99, and the maximum price is $1700.00.\n",
    "The mean price is approximately $179.40, with a standard deviation of approximately $328.60.\n",
    "The median (50th percentile) price is $14.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = dap_sept.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OCTOBER 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_oct.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_oct.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_oct.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 126 duplicate rows in the October DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_oct.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the October dataset, there are missing values in several columns:\n",
    "\n",
    "95 missing values in the 'Order_ID', 'Quantity_Ordered', 'Price_Each', and 'Order_Date' columns.\n",
    "62 missing values in the 'Product' and 'Purchase_Address' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "dap_oct.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The October dataset has 20,284 entries. For the 'Order_ID' column, the minimum value is 259,358, and the maximum value is 278,796. The 'Quantity_Ordered' column has a mean of approximately 1.12, with a standard deviation of around 0.44. In the 'Price_Each' column, the mean price is approximately $183.18, with a standard deviation of approximately $334.00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = dap_oct.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOVEMBER 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_nov.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_nov.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_nov.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the November DataFrame, there are 108 duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_nov.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the November DataFrame, there are missing values in several columns:\n",
    "\n",
    "Order_ID: 81 missing values\n",
    "\n",
    "Product: 45 missing values\n",
    "\n",
    "Quantity_Ordered: 81 missing values\n",
    "\n",
    "Price_Each: 81 missing values\n",
    "\n",
    "Order_Date: 81 missing values\n",
    "\n",
    "Purchase_Address: 45 missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "dap_nov.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The November DataFrame consists of 17,580 entries. Here's a summary of its descriptive statistics:\n",
    "\n",
    "1. Order_ID: Ranges from 278797 to 295664 with a mean of approximately 287,236. The standard deviation is around 4,867, indicating a moderate spread of data around the mean.\n",
    "2. Quantity_Ordered: The average quantity ordered is about 1.13, with a standard deviation of approximately 0.45. The quantity ranges from 1 to 8 units.\n",
    "3. Price_Each: Prices range from $2.99 to $1700. The mean price is approximately $180.88, with a standard deviation of about $330. This wide spread indicates considerable variability in item prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = dap_nov.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECEMBER 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_dec.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_dec.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_dec.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In December's dataset, there are 166 duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_dec.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In December's dataset:\n",
    "\n",
    "1. Order_ID: There are 128 missing values.\n",
    "2. Product: There are 80 missing values.\n",
    "3. Quantity_Ordered: There are 128 missing values.\n",
    "4. Price_Each: There are 128 missing values.\n",
    "5. Order_Date: There are 128 missing values.\n",
    "6. Purchase_Address: There are 80 missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "dap_dec.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Order_ID: There are 24989 orders in total.\n",
    "2. Quantity_Ordered: The average quantity ordered is approximately 1.13, with a standard deviation of around 0.45. The minimum quantity ordered is 1, and the maximum is 7.\n",
    "3. Price_Each: The mean price per item is approximately $183.85, with a standard deviation of approximately $333.08. The prices range from $2.99 to $1700."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = dap_dec.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "January"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_January_2019.dropna(subset=df_sales_January_2019.columns[df_sales_January_2019.isnull().any()], how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = df_sales_January_2019.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "df_sales_January_2019.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and reset index inplace\n",
    "df_sales_January_2019.drop_duplicates(inplace=True)\n",
    "df_sales_January_2019.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows in the DataFrame\n",
    "df_sales_January_2019.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order ID' column to numeric, coercing errors to NaN\n",
    "df_sales_January_2019['Order ID'] = pd.to_numeric(df_sales_January_2019['Order ID'], errors='coerce')\n",
    "\n",
    "# Convert 'Quantity Ordered' and 'Price Each' columns to numeric\n",
    "df_sales_January_2019['Quantity Ordered'] = pd.to_numeric(df_sales_January_2019['Quantity Ordered'], errors='coerce')\n",
    "df_sales_January_2019['Price Each'] = pd.to_numeric(df_sales_January_2019['Price Each'], errors='coerce')\n",
    "\n",
    "# Convert 'Order Date' column to datetime\n",
    "df_sales_January_2019['Order Date'] = pd.to_datetime(df_sales_January_2019['Order Date'], format='%m/%d/%y %H:%M', errors='coerce')\n",
    "\n",
    "\n",
    "# Now, check the data types after conversion\n",
    "print(df_sales_January_2019.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and print the number of unique values in each column\n",
    "for column in df_sales_January_2019.columns:\n",
    "    unique_values = df_sales_January_2019[column].nunique()\n",
    "    print(f\"Number of unique values in '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and view unique values\n",
    "for column in df_sales_January_2019.columns:\n",
    "    unique_values = df_sales_January_2019[column].unique()\n",
    "    print(f\"Unique values in '{column}':\")\n",
    "    print(unique_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Order ID: The 'Order ID' column contains unique numerical identifiers for each order. There are no apparent issues with this column.\n",
    "2. Product: The 'Product' column contains the names of various products sold. It includes a variety of products such as iPhones, charging cables, headphones, monitors, batteries, laptops, TVs, and washing machines. However, there are two unusual entries: 'nan' and 'Product', which might indicate missing or placeholder values.\n",
    "3. Quantity Ordered: The 'Quantity Ordered' column contains the number of units ordered for each product. Most entries are numerical values representing quantities, but there are some unusual entries such as 'nan' and 'Quantity Ordered', which may indicate missing or placeholder values.\n",
    "4. Price Each: The 'Price Each' column contains the price of each product. Most entries are numerical values representing prices, but there are some unusual entries such as 'nan' and 'Price Each', which may indicate missing or placeholder values. Additionally, there are some duplicate values in a different format ('600' and '600.0').\n",
    "5. Order Date: The 'Order Date' column contains the date and time when each order was placed. Entries are in the format 'MM/DD/YY HH:mm'.\n",
    "6. Purchase Address: The 'Purchase Address' column contains the addresses where the purchases were made. Each entry includes the street address, city, and ZIP code.\n",
    "\n",
    "Overall, while most columns appear to contain relevant information, there are some anomalies in the 'Product', 'Quantity Ordered', and 'Price Each' columns that need further investigation and potential cleaning. Additionally, missing values ('nan') and placeholder values ('Product' and 'Quantity Ordered') should be addressed to ensure the integrity of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "February"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_February_2019.dropna(subset=df_sales_February_2019.columns[df_sales_February_2019.isnull().any()], how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = df_sales_February_2019.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "df_sales_February_2019.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and reset index inplace\n",
    "df_sales_February_2019.drop_duplicates(inplace=True)\n",
    "df_sales_February_2019.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows in the DataFrame\n",
    "df_sales_February_2019.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order ID' column to numeric, coercing errors to NaN\n",
    "df_sales_February_2019['Order ID'] = pd.to_numeric(df_sales_February_2019['Order ID'], errors='coerce')\n",
    "\n",
    "# Convert 'Quantity Ordered' and 'Price Each' columns to numeric\n",
    "df_sales_February_2019['Quantity Ordered'] = pd.to_numeric(df_sales_February_2019['Quantity Ordered'], errors='coerce')\n",
    "df_sales_February_2019['Price Each'] = pd.to_numeric(df_sales_February_2019['Price Each'], errors='coerce')\n",
    "\n",
    "# Convert 'Order Date' column to datetime\n",
    "df_sales_February_2019['Order Date'] = pd.to_datetime(df_sales_February_2019['Order Date'], format='%m/%d/%y %H:%M', errors='coerce')\n",
    "\n",
    "\n",
    "# Now, check the data types after conversion\n",
    "print(df_sales_February_2019.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and print the number of unique values in each column\n",
    "for column in df_sales_February_2019.columns:\n",
    "    unique_values = df_sales_February_2019[column].nunique()\n",
    "    print(f\"Number of unique values in '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and view unique values\n",
    "for column in df_sales_February_2019.columns:\n",
    "    unique_values = df_sales_February_2019[column].unique()\n",
    "    print(f\"Unique values in '{column}':\")\n",
    "    print(unique_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Order ID: The 'Order ID' column contains unique numerical identifiers for each order. There are no apparent issues with this column.\n",
    "2. Product: The 'Product' column contains the names of various products sold. It includes a variety of products such as iPhones, batteries, monitors, charging cables, headphones, laptops, TVs, and washing machines. However, there are two unusual entries: 'nan' and 'Product', which might indicate missing or placeholder values.\n",
    "3. Quantity Ordered: The 'Quantity Ordered' column contains the number of units ordered for each product. Most entries are numerical values representing quantities, but there are some unusual entries such as 'nan' and 'Quantity Ordered', which may indicate missing or placeholder values.\n",
    "4. Price Each: The 'Price Each' column contains the price of each product. Most entries are numerical values representing prices, but there are some unusual entries such as 'nan' and 'Price Each', which may indicate missing or placeholder values. Additionally, there are some duplicate values in a different format ('600' and '600.0').\n",
    "5. Order Date: The 'Order Date' column contains the date and time when each order was placed. Entries are in the format 'MM/DD/YY HH:mm'.\n",
    "6. Purchase Address: The 'Purchase Address' column contains the addresses where the purchases were made. Each entry includes the street address, city, state, and ZIP code.\n",
    "\n",
    "Similar to the previous observations, anomalies exist in the 'Product', 'Quantity Ordered', and 'Price Each' columns that need further investigation and cleaning. Additionally, missing values ('nan') and placeholder values ('Product' and 'Quantity Ordered') should be handled appropriately to ensure the accuracy of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "March"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_March_2019.dropna(subset=df_sales_March_2019.columns[df_sales_March_2019.isnull().any()], how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = df_sales_March_2019.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "df_sales_March_2019.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and reset index inplace\n",
    "df_sales_March_2019.drop_duplicates(inplace=True)\n",
    "df_sales_March_2019.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows in the DataFrame\n",
    "df_sales_March_2019.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order ID' column to numeric, coercing errors to NaN\n",
    "df_sales_March_2019['Order ID'] = pd.to_numeric(df_sales_March_2019['Order ID'], errors='coerce')\n",
    "\n",
    "# Convert 'Quantity Ordered' and 'Price Each' columns to numeric\n",
    "df_sales_March_2019['Quantity Ordered'] = pd.to_numeric(df_sales_March_2019['Quantity Ordered'], errors='coerce')\n",
    "df_sales_March_2019['Price Each'] = pd.to_numeric(df_sales_March_2019['Price Each'], errors='coerce')\n",
    "\n",
    "# Convert 'Order Date' column to datetime\n",
    "df_sales_March_2019['Order Date'] = pd.to_datetime(df_sales_March_2019['Order Date'], format='%m/%d/%y %H:%M', errors='coerce')\n",
    "\n",
    "\n",
    "# Now, check the data types after conversion\n",
    "print(df_sales_March_2019.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and print the number of unique values in each column\n",
    "for column in df_sales_March_2019.columns:\n",
    "    unique_values = df_sales_March_2019[column].nunique()\n",
    "    print(f\"Number of unique values in '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and view unique values\n",
    "for column in df_sales_March_2019.columns:\n",
    "    unique_values = df_sales_March_2019[column].unique()\n",
    "    print(f\"Unique values in '{column}':\")\n",
    "    print(unique_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Order ID: The 'Order ID' column contains unique numerical identifiers for each order. There are no apparent issues with this column.\n",
    "2. Product: The 'Product' column contains the names of various products sold. It includes a variety of products such as iPhones, charging cables, headphones, monitors, batteries, laptops, TVs, and washing machines. However, there are two unusual entries: 'nan' and 'Product', which might indicate missing or placeholder values.\n",
    "3. Quantity Ordered: The 'Quantity Ordered' column contains the number of units ordered for each product. Most entries are numerical values representing quantities, but there are some unusual entries such as 'nan' and 'Quantity Ordered', which may indicate missing or placeholder values.\n",
    "4. Price Each: The 'Price Each' column contains the price of each product. Most entries are numerical values representing prices, but there are some unusual entries such as 'nan' and 'Price Each', which may indicate missing or placeholder values. Additionally, there are some duplicate values in a different format ('600' and '600.0').\n",
    "5. Order Date: The 'Order Date' column contains the date and time when each order was placed. Entries are in the format 'MM/DD/YY HH:mm'.\n",
    "6. Purchase Address: The 'Purchase Address' column contains the addresses where the purchases were made. Each entry includes the street address, city, state, and ZIP code.\n",
    "\n",
    "Similar to the previous observations, anomalies exist in the 'Product', 'Quantity Ordered', and 'Price Each' columns that need further investigation and cleaning. Additionally, missing values ('nan') and placeholder values ('Product' and 'Quantity Ordered') should be handled appropriately to ensure the accuracy of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "April"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_April_2019.dropna(subset=df_sales_April_2019.columns[df_sales_April_2019.isnull().any()], how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = df_sales_April_2019.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "df_sales_April_2019.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and reset index inplace\n",
    "df_sales_April_2019.drop_duplicates(inplace=True)\n",
    "df_sales_April_2019.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows in the DataFrame\n",
    "df_sales_April_2019.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order ID' column to numeric, coercing errors to NaN\n",
    "df_sales_April_2019['Order ID'] = pd.to_numeric(df_sales_April_2019['Order ID'], errors='coerce')\n",
    "\n",
    "# Convert 'Quantity Ordered' and 'Price Each' columns to numeric\n",
    "df_sales_April_2019['Quantity Ordered'] = pd.to_numeric(df_sales_April_2019['Quantity Ordered'], errors='coerce')\n",
    "df_sales_April_2019['Price Each'] = pd.to_numeric(df_sales_April_2019['Price Each'], errors='coerce')\n",
    "\n",
    "# Convert 'Order Date' column to datetime\n",
    "df_sales_April_2019['Order Date'] = pd.to_datetime(df_sales_April_2019['Order Date'], format='%m/%d/%y %H:%M', errors='coerce')\n",
    "\n",
    "\n",
    "# Now, check the data types after conversion\n",
    "print(df_sales_April_2019.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and print the number of unique values in each column\n",
    "for column in df_sales_April_2019.columns:\n",
    "    unique_values = df_sales_April_2019[column].nunique()\n",
    "    print(f\"Number of unique values in '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and view unique values\n",
    "for column in df_sales_April_2019:\n",
    "    unique_values = df_sales_April_2019[column].unique()\n",
    "    print(f\"Unique values in '{column}':\")\n",
    "    print(unique_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Order ID: The 'Order ID' column contains unique numerical identifiers for each order. However, there are some 'nan' values present in this column, indicating missing values.\n",
    "2. Product: The 'Product' column contains the names of various products sold. Similar to previous observations, there are 'nan' and 'Product' entries, indicating missing or placeholder values.\n",
    "3. Quantity Ordered: The 'Quantity Ordered' column contains the number of units ordered for each product. Similar to previous observations, there are 'nan' and 'Quantity Ordered' entries, indicating missing or placeholder values.\n",
    "4. Price Each: The 'Price Each' column contains the price of each product. Similar to previous observations, there are 'nan' and 'Price Each' entries, indicating missing or placeholder values. Additionally, there are some duplicate values in a different format ('600' and '600.0').\n",
    "5. Order Date: The 'Order Date' column contains the date and time when each order was placed. Similar to previous observations, there are 'nan' entries, indicating missing values.\n",
    "6. Purchase Address: The 'Purchase Address' column contains the addresses where the purchases were made. Similar to previous observations, there are 'nan' entries, indicating missing values.\n",
    "\n",
    "As before, the anomalies in the 'Product', 'Quantity Ordered', 'Price Each', 'Order Date', and 'Purchase Address' columns need further investigation and cleaning to ensure the accuracy of the dataset. Additionally, missing values should be handled appropriately to maintain data integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_May_2019.dropna(subset=df_sales_May_2019.columns[df_sales_May_2019.isnull().any()], how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = df_sales_May_2019.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "df_sales_May_2019.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and reset index inplace\n",
    "df_sales_May_2019.drop_duplicates(inplace=True)\n",
    "df_sales_May_2019.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows in the DataFrame\n",
    "df_sales_May_2019.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order ID' column to numeric, coercing errors to NaN\n",
    "df_sales_May_2019['Order ID'] = pd.to_numeric(df_sales_May_2019['Order ID'], errors='coerce')\n",
    "\n",
    "# Convert 'Quantity Ordered' and 'Price Each' columns to numeric\n",
    "df_sales_May_2019['Quantity Ordered'] = pd.to_numeric(df_sales_May_2019['Quantity Ordered'], errors='coerce')\n",
    "df_sales_May_2019['Price Each'] = pd.to_numeric(df_sales_May_2019['Price Each'], errors='coerce')\n",
    "\n",
    "# Convert 'Order Date' column to datetime\n",
    "df_sales_May_2019['Order Date'] = pd.to_datetime(df_sales_May_2019['Order Date'], format='%m/%d/%y %H:%M', errors='coerce')\n",
    "\n",
    "\n",
    "# Now, check the data types after conversion\n",
    "print(df_sales_May_2019.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and print the number of unique values in each column\n",
    "for column in df_sales_May_2019.columns:\n",
    "    unique_values = df_sales_May_2019[column].nunique()\n",
    "    print(f\"Number of unique values in '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and view unique values\n",
    "for column in df_sales_May_2019:\n",
    "    unique_values = df_sales_May_2019[column].unique()\n",
    "    print(f\"Unique values in '{column}':\")\n",
    "    print(unique_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Order ID: The 'Order ID' column contains unique numerical identifiers for each order. There are no apparent issues with this column.\n",
    "2. Product: The 'Product' column contains the names of various products sold. However, there are two unusual entries: 'nan' and 'Product', which might indicate missing or placeholder values.\n",
    "3. Quantity Ordered: The 'Quantity Ordered' column contains the number of units ordered for each product. Most entries are numerical values representing quantities, but there are some unusual entries such as 'nan' and 'Quantity Ordered', which may indicate missing or placeholder values.\n",
    "4. Price Each: The 'Price Each' column contains the price of each product. Most entries are numerical values representing prices, but there are some unusual entries such as 'nan' and 'Price Each', which may indicate missing or placeholder values. Additionally, there are some duplicate values in a different format ('600' and '600.0').\n",
    "5. Order Date: The 'Order Date' column contains the date and time when each order was placed. Entries are in the format 'MM/DD/YY HH:mm'.\n",
    "6. Purchase Address: The 'Purchase Address' column contains the addresses where the purchases were made. Each entry includes the street address, city, state, and ZIP code.\n",
    "\n",
    "Similar to the previous observations, anomalies exist in the 'Product', 'Quantity Ordered', and 'Price Each' columns that need further investigation and cleaning. Additionally, missing values ('nan') and placeholder values ('Product' and 'Quantity Ordered') should be handled appropriately to ensure the accuracy of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "June"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_June_2019.dropna(subset=df_sales_June_2019.columns[df_sales_June_2019.isnull().any()], how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = df_sales_June_2019.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "df_sales_June_2019.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and reset index inplace\n",
    "df_sales_June_2019.drop_duplicates(inplace=True)\n",
    "df_sales_June_2019.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows in the DataFrame\n",
    "df_sales_June_2019.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order ID' column to numeric, coercing errors to NaN\n",
    "df_sales_June_2019['Order ID'] = pd.to_numeric(df_sales_June_2019['Order ID'], errors='coerce')\n",
    "\n",
    "# Convert 'Quantity Ordered' and 'Price Each' columns to numeric\n",
    "df_sales_June_2019['Quantity Ordered'] = pd.to_numeric(df_sales_June_2019['Quantity Ordered'], errors='coerce')\n",
    "df_sales_June_2019['Price Each'] = pd.to_numeric(df_sales_June_2019['Price Each'], errors='coerce')\n",
    "\n",
    "# Convert 'Order Date' column to datetime\n",
    "df_sales_June_2019['Order Date'] = pd.to_datetime(df_sales_June_2019['Order Date'], format='%m/%d/%y %H:%M', errors='coerce')\n",
    "\n",
    "\n",
    "# Now, check the data types after conversion\n",
    "print(df_sales_June_2019.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the column names\n",
    "df_sales_June_2019.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and print the number of unique values in each column\n",
    "for column in df_sales_June_2019:\n",
    "    unique_values = df_sales_June_2019[column].nunique()\n",
    "    print(f\"Number of unique values in '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and view unique values\n",
    "for column in df_sales_June_2019:\n",
    "    unique_values = df_sales_June_2019[column].unique()\n",
    "    print(f\"Unique values in '{column}':\")\n",
    "    print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Order ID: The 'Order ID' column contains unique numerical identifiers for each order. There are no apparent issues with this column.\n",
    "2. Product: The 'Product' column contains the names of various products sold. However, there are two unusual entries: 'nan' and 'Product', which might indicate missing or placeholder values.\n",
    "3. Quantity Ordered: The 'Quantity Ordered' column contains the number of units ordered for each product. Most entries are numerical values representing quantities, but there are some unusual entries such as 'nan' and 'Quantity Ordered', which may indicate missing or placeholder values.\n",
    "4. Price Each: The 'Price Each' column contains the price of each product. Most entries are numerical values representing prices, but there are some unusual entries such as 'nan' and 'Price Each', which may indicate missing or placeholder values. Additionally, there are some duplicate values in a different format ('600' and '600.0').\n",
    "5. Order Date: The 'Order Date' column contains the date and time when each order was placed. Entries are in the format 'MM/DD/YY HH:mm'.\n",
    "6. Purchase Address: The 'Purchase Address' column contains the addresses where the purchases were made. Each entry includes the street address, city, state, and ZIP code.\n",
    "\n",
    "Similar to the previous observations, anomalies exist in the 'Product', 'Quantity Ordered', and 'Price Each' columns that need further investigation and cleaning. Additionally, missing values ('nan') and placeholder values ('Product' and 'Quantity Ordered') should be handled appropriately to ensure the accuracy of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "July"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_july.dropna(subset=dap_july.columns[dap_july.isnull().any()], how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = dap_july.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_july.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and reset index inplace\n",
    "dap_july.drop_duplicates(inplace=True)\n",
    "dap_july.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows in the DataFrame\n",
    "dap_july.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new column names\n",
    "new_column_names = {\n",
    "    'Order_ID': 'Order ID',\n",
    "    'Quantity_Ordered': 'Quantity Ordered',\n",
    "    'Price_Each': 'Price Each',\n",
    "    'Order_Date': 'Order Date',\n",
    "    'Purchase_Address': 'Purchase Address'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "dap_july.rename(columns=new_column_names, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order Date' column to datetime\n",
    "dap_july['Order Date'] = pd.to_datetime(dap_july['Order Date'], format='%m/%d/%y %H:%M', errors='coerce')\n",
    "\n",
    "\n",
    "# Now, check the data types after conversion\n",
    "print(dap_july.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and print the number of unique values in each column\n",
    "for column in dap_july:\n",
    "    unique_values = dap_july[column].nunique()\n",
    "    print(f\"Number of unique values in '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and view unique values\n",
    "for column in dap_july.columns:\n",
    "    unique_values = dap_july[column].unique()\n",
    "    print(f\"Unique values in '{column}':\")\n",
    "    print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Order_ID: The 'Order_ID' column contains unique numerical identifiers for each order. There are no apparent issues with this column.\n",
    "2. Product: The 'Product' column contains the names of various products sold. However, there are two unusual entries: 'None' and 'Product', which might indicate missing or placeholder values.\n",
    "3. Quantity_Ordered: The 'Quantity_Ordered' column contains the number of units ordered for each product. Most entries are numerical values representing quantities, but there are some missing values (NaN) present.\n",
    "4. Price_Each: The 'Price_Each' column contains the price of each product. Most entries are numerical values representing prices, but there are some missing values (NaN) present.\n",
    "5. Order_Date: The 'Order_Date' column contains the date and time when each order was placed. Entries seem to be in datetime format, but there might be some inconsistencies in the date format.\n",
    "6. Purchase_Address: The 'Purchase_Address' column contains the addresses where the purchases were made. Each entry includes the street address, city, state, and ZIP code.\n",
    "\n",
    "There seem to be some inconsistencies in the data, such as different date formats in the 'Order_Date' column and missing values in the 'Quantity_Ordered' and 'Price_Each' columns. Additionally, the 'Product' column contains 'None' and 'Product' entries, which need to be addressed. Further data cleaning and preprocessing may be necessary to ensure the integrity of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with a default date (e.g., December 1, 2019)\n",
    "default_date_jul = pd.to_datetime('2019-07-01', format='%Y-%m-%d')\n",
    "dap_july['Order Date'] = pd.to_datetime(dap_july['Order Date'], format='%d/%m/%Y %H:%M').fillna(default_date_jul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_july.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_august.dropna(subset=dap_august.columns[dap_august.isnull().any()], how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = dap_august.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_august.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and reset index inplace\n",
    "dap_august.drop_duplicates(inplace=True)\n",
    "dap_august.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows in the DataFrame\n",
    "dap_august.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new column names\n",
    "new_column_names = {\n",
    "    'Order_ID': 'Order ID',\n",
    "    'Quantity_Ordered': 'Quantity Ordered',\n",
    "    'Price_Each': 'Price Each',\n",
    "    'Order_Date': 'Order Date',\n",
    "    'Purchase_Address': 'Purchase Address'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "dap_august.rename(columns=new_column_names, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order Date' column to datetime\n",
    "dap_august['Order Date'] = pd.to_datetime(dap_august['Order Date'], format='%m/%d/%y %H:%M', errors='coerce')\n",
    "\n",
    "\n",
    "# Now, check the data types after conversion\n",
    "print(dap_august.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the column names\n",
    "dap_august.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and print the number of unique values in each column\n",
    "for column in dap_august.columns:\n",
    "    unique_values = dap_august[column].nunique()\n",
    "    print(f\"Number of unique values in '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and view unique values\n",
    "for column in dap_august.columns:\n",
    "    unique_values = dap_august[column].unique()\n",
    "    print(f\"Unique values in '{column}':\")\n",
    "    print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Order_ID: The 'Order_ID' column contains unique numerical identifiers for each order. There are no apparent issues with this column.\n",
    "2. Product: The 'Product' column contains the names of various products sold. However, there are two unusual entries: 'None' and 'Product', which might indicate missing or placeholder values.\n",
    "3. Quantity_Ordered: The 'Quantity_Ordered' column contains the number of units ordered for each product. Most entries are numerical values representing quantities, but there are some missing values (NaN) present.\n",
    "4. Price_Each: The 'Price_Each' column contains the price of each product. Most entries are numerical values representing prices, but there are some missing values (NaN) present.\n",
    "5. Order_Date: The 'Order_Date' column contains the date and time when each order was placed. Entries seem to be in datetime format.\n",
    "6. Purchase_Address: The 'Purchase_Address' column contains the addresses where the purchases were made. Each entry includes the street address, city, state, and ZIP code.\n",
    "\n",
    "The data seems to have similar issues as before, with missing values in the 'Quantity_Ordered' and 'Price_Each' columns, and inconsistent entries in the 'Product' column. Additionally, there appear to be no missing values in the 'Order_ID', 'Order_Date', and 'Purchase_Address' columns. Further cleaning and preprocessing may be necessary to ensure the integrity of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with a default date (e.g., December 1, 2019)\n",
    "default_date_aug = pd.to_datetime('2019-08-01', format='%Y-%m-%d')\n",
    "dap_august['Order Date'] = pd.to_datetime(dap_august['Order Date'], format='%d/%m/%Y %H:%M').fillna(default_date_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_august.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "September"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_sept.dropna(subset=dap_sept.columns[dap_sept.isnull().any()], how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = dap_sept.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_sept.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and reset index inplace\n",
    "dap_sept.drop_duplicates(inplace=True)\n",
    "dap_sept.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows in the DataFrame\n",
    "dap_sept.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new column names\n",
    "new_column_names = {\n",
    "    'Order_ID': 'Order ID',\n",
    "    'Quantity_Ordered': 'Quantity Ordered',\n",
    "    'Price_Each': 'Price Each',\n",
    "    'Order_Date': 'Order Date',\n",
    "    'Purchase_Address': 'Purchase Address'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "dap_sept.rename(columns=new_column_names, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order Date' column to datetime\n",
    "dap_sept['Order Date'] = pd.to_datetime(dap_sept['Order Date'], format='%m/%d/%y %H:%M', errors='coerce')\n",
    "\n",
    "\n",
    "# Now, check the data types after conversion\n",
    "print(dap_sept.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the column names\n",
    "dap_sept.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and print the number of unique values in each column\n",
    "for column in dap_sept:\n",
    "    unique_values = dap_sept[column].nunique()\n",
    "    print(f\"Number of unique values in '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and view unique values\n",
    "for column in dap_sept.columns:\n",
    "    unique_values = dap_sept[column].unique()\n",
    "    print(f\"Unique values in '{column}':\")\n",
    "    print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Order_ID: The 'Order_ID' column contains unique numerical identifiers for each order. There are no apparent issues with this column.\n",
    "2. Product: The 'Product' column contains the names of various products sold. However, there are two unusual entries: 'None' and 'Product', which might indicate missing or placeholder values.\n",
    "3. Quantity_Ordered: The 'Quantity_Ordered' column contains the number of units ordered for each product. Most entries are numerical values representing quantities, but there are some missing values (NaN) present.\n",
    "4. Price_Each: The 'Price_Each' column contains the price of each product. Most entries are numerical values representing prices, but there are some missing values (NaN) present.\n",
    "5. Order_Date: The 'Order_Date' column contains the date and time when each order was placed. Entries seem to be in datetime format.\n",
    "6. Purchase_Address: The 'Purchase_Address' column contains the addresses where the purchases were made. Each entry includes the street address, city, state, and ZIP code.\n",
    "\n",
    "Similar to previous observations, the data requires cleaning and preprocessing to address missing values and inconsistent entries in the 'Product' column. The 'Order_ID', 'Order_Date', and 'Purchase_Address' columns seem to have no missing values. Further data cleaning and preprocessing may be necessary to ensure the integrity of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with a default date (e.g., December 1, 2019)\n",
    "default_date_sept = pd.to_datetime('2019-09-01', format='%Y-%m-%d')\n",
    "dap_sept['Order Date'] = pd.to_datetime(dap_sept['Order Date'], format='%d/%m/%Y %H:%M').fillna(default_date_sept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_sept.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "October"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_oct.dropna(subset=dap_oct.columns[dap_oct.isnull().any()], how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = dap_oct.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_oct.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and reset index inplace\n",
    "dap_oct.drop_duplicates(inplace=True)\n",
    "dap_oct.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows in the DataFrame\n",
    "dap_oct.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new column names\n",
    "new_column_names = {\n",
    "    'Order_ID': 'Order ID',\n",
    "    'Quantity_Ordered': 'Quantity Ordered',\n",
    "    'Price_Each': 'Price Each',\n",
    "    'Order_Date': 'Order Date',\n",
    "    'Purchase_Address': 'Purchase Address'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "dap_oct.rename(columns=new_column_names, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order Date' column to datetime\n",
    "dap_oct['Order Date'] = pd.to_datetime(dap_oct['Order Date'], format='%m/%d/%y %H:%M', errors='coerce')\n",
    "\n",
    "\n",
    "# Now, check the data types after conversion\n",
    "print(dap_oct.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the column names\n",
    "dap_oct.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and print the number of unique values in each column\n",
    "for column in dap_oct.columns:\n",
    "    unique_values = dap_oct[column].nunique()\n",
    "    print(f\"Number of unique values in '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and view unique values\n",
    "for column in dap_oct.columns:\n",
    "    unique_values = dap_oct[column].unique()\n",
    "    print(f\"Unique values in '{column}':\")\n",
    "    print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Order_ID: The 'Order_ID' column contains unique numerical identifiers for each order. There are no apparent issues with this column.\n",
    "2. Product: The 'Product' column contains the names of various products sold. However, there are two unusual entries: 'None' and 'Product', which might indicate missing or placeholder values.\n",
    "3. Quantity_Ordered: The 'Quantity_Ordered' column contains the number of units ordered for each product. Most entries are numerical values representing quantities, but there are some missing values (NaN) present.\n",
    "4. Price_Each: The 'Price_Each' column contains the price of each product. Most entries are numerical values representing prices, but there are some missing values (NaN) present.\n",
    "5. Order_Date: The 'Order_Date' column contains the date and time when each order was placed. Entries seem to be in datetime format.\n",
    "6. Purchase_Address: The 'Purchase_Address' column contains the addresses where the purchases were made. Each entry includes the street address, city, state, and ZIP code.\n",
    "\n",
    "Similar to previous observations, the data requires cleaning and preprocessing to address missing values and inconsistent entries in the 'Product' column. The 'Order_ID', 'Order_Date', and 'Purchase_Address' columns seem to have no missing values. Further data cleaning and preprocessing may be necessary to ensure the integrity of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with a default date (e.g., December 1, 2019)\n",
    "default_date_oct = pd.to_datetime('2019-10-01', format='%Y-%m-%d')\n",
    "dap_oct['Order Date'] = pd.to_datetime(dap_oct['Order Date'], format='%d/%m/%Y %H:%M').fillna(default_date_oct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_oct.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "November"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_nov.dropna(subset=dap_nov.columns[dap_nov.isnull().any()], how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = dap_nov.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_nov.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_nov.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and reset index inplace\n",
    "dap_nov.drop_duplicates(inplace=True)\n",
    "dap_nov.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows in the DataFrame\n",
    "dap_nov.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new column names\n",
    "new_column_names = {\n",
    "   'Order_ID': 'Order ID',\n",
    "    'Quantity_Ordered': 'Quantity Ordered',\n",
    "    'Price_Each': 'Price Each',\n",
    "    'Order_Date': 'Order Date',\n",
    "    'Purchase_Address': 'Purchase Address'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "dap_nov.rename(columns=new_column_names, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order Date' column to datetime\n",
    "dap_nov['Order Date'] = pd.to_datetime(dap_nov['Order Date'], format='%m/%d/%y %H:%M', errors='coerce')\n",
    "\n",
    "\n",
    "# Now, check the data types after conversion\n",
    "print(dap_nov.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the column names\n",
    "dap_nov.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and print the number of unique values in each column\n",
    "for column in dap_nov.columns:\n",
    "    unique_values = dap_nov[column].nunique()\n",
    "    print(f\"Number of unique values in '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and view unique values\n",
    "for column in dap_nov.columns:\n",
    "    unique_values = dap_nov[column].unique()\n",
    "    print(f\"Unique values in '{column}':\")\n",
    "    print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Order_ID: The 'Order_ID' column contains unique numerical identifiers for each order, and there are no apparent issues with this column.\n",
    "2. Product: The 'Product' column contains the names of various products sold. There is an entry 'None', which might indicate missing or placeholder values. Additionally, there is an entry 'Product', which seems like a placeholder or a mistake.\n",
    "3. Quantity_Ordered: The 'Quantity_Ordered' column contains the number of units ordered for each product. Most entries are numerical values representing quantities, but there are some missing values (NaN) present.\n",
    "4. Price_Each: The 'Price_Each' column contains the price of each product. Most entries are numerical values representing prices, but there are some missing values (NaN) present.\n",
    "5. Order_Date: The 'Order_Date' column contains the date and time when each order was placed. Entries seem to be in datetime format.\n",
    "6. Purchase_Address: The 'Purchase_Address' column contains the addresses where the purchases were made. Each entry includes the street address, city, state, and ZIP code.\n",
    "\n",
    "Similar to previous observations, the data requires cleaning and preprocessing to address missing values and inconsistent entries in the 'Product' column. The 'Order_ID', 'Order_Date', and 'Purchase_Address' columns seem to have no missing values. Further data cleaning and preprocessing may be necessary to ensure the integrity of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with a default date (e.g., December 1, 2019)\n",
    "default_date_nov = pd.to_datetime('2019-11-01', format='%Y-%m-%d')\n",
    "dap_nov['Order Date'] = pd.to_datetime(dap_nov['Order Date'], format='%d/%m/%Y %H:%M').fillna(default_date_nov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_nov.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_dec.dropna(subset=dap_dec.columns[dap_dec.isnull().any()], how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = dap_dec.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_dec.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows and reset index inplace\n",
    "dap_dec.drop_duplicates(inplace=True)\n",
    "dap_dec.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows in the DataFrame\n",
    "dap_dec.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new column names\n",
    "new_column_names = {\n",
    "    'Order_ID': 'Order ID',\n",
    "    'Quantity_Ordered': 'Quantity Ordered',\n",
    "    'Price_Each': 'Price Each',\n",
    "    'Order_Date': 'Order Date',\n",
    "    'Purchase_Address': 'Purchase Address'\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "dap_dec.rename(columns=new_column_names, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order Date' column to datetime\n",
    "dap_dec['Order Date'] = pd.to_datetime(dap_dec['Order Date'], format='%m/%d/%y %H:%M', errors='coerce')\n",
    "\n",
    "\n",
    "# Now, check the data types after conversion\n",
    "print(dap_dec.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the column names\n",
    "dap_dec.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and print the number of unique values in each column\n",
    "for column in dap_dec.columns:\n",
    "    unique_values = dap_dec[column].nunique()\n",
    "    print(f\"Number of unique values in '{column}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over columns and view unique values\n",
    "for column in dap_dec.columns:\n",
    "    unique_values = dap_dec[column].unique()\n",
    "    print(f\"Unique values in '{column}':\")\n",
    "    print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Order_ID: The 'Order_ID' column contains unique numerical identifiers for each order, and there are no apparent issues with this column.\n",
    "2. Product: The 'Product' column contains the names of various products sold. There is an entry 'None', which might indicate missing or placeholder values. Additionally, there is an entry 'Product', which seems like a placeholder or a mistake.\n",
    "3. Quantity_Ordered: The 'Quantity_Ordered' column contains the number of units ordered for each product. Most entries are numerical values representing quantities, but there are some missing values (NaN) present.\n",
    "4. Price_Each: The 'Price_Each' column contains the price of each product. Most entries are numerical values representing prices, but there are some missing values (NaN) present.\n",
    "5. Order_Date: The 'Order_Date' column contains the date and time when each order was placed. Entries seem to be in datetime format.\n",
    "6. Purchase_Address: The 'Purchase_Address' column contains the addresses where the purchases were made. Each entry includes the street address, city, state, and ZIP code.\n",
    "\n",
    "Similar to previous observations, the data requires cleaning and preprocessing to address missing values and inconsistent entries in the 'Product' column. The 'Order_ID', 'Order_Date', and 'Purchase_Address' columns seem to have no missing values. Further data cleaning and preprocessing may be necessary to ensure the integrity of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with a default date (e.g., December 1, 2019)\n",
    "default_date_dec = pd.to_datetime('2019-12-01', format='%Y-%m-%d')\n",
    "dap_dec['Order Date'] = pd.to_datetime(dap_dec['Order Date'], format='%d/%m/%Y %H:%M').fillna(default_date_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values for First dataset\n",
    "dap_dec.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMBINING DATASETS\n",
    "\n",
    "SQL Dataset combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_combined = pd.concat([dap_july, dap_august,dap_sept,dap_oct,dap_nov,dap_dec], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dap_combined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine sales_data using your actual DataFrame\n",
    "sales_data = pd.concat([df_sales_January_2019, df_sales_February_2019, df_sales_March_2019, df_sales_April_2019, df_sales_May_2019, df_sales_June_2019, dap_combined],ignore_index=True)\n",
    "#dap_july, dap_august, dap_sept, dap_oct, dap_nov, dap_dec\n",
    "\n",
    "# Define a dictionary mapping each product to its category\n",
    "product_category_mapping = {\n",
    "    'Macbook Pro Laptop': 'Electronics',\n",
    "    'LG Washing Machine': 'Electronics',\n",
    "    '27in FHD Monitor': 'Electronics',\n",
    "    'ThinkPad Laptop': 'Electronics',\n",
    "    'Google Phone': 'Electronics',\n",
    "    'Apple Airpods Headphones': 'Electronics',\n",
    "    'Vareebadd Phone': 'Electronics',\n",
    "    'iPhone': 'Electronics',\n",
    "    '20in Monitor': 'Electronics',\n",
    "    '34in Ultrawide Monitor': 'Electronics',\n",
    "    '27in 4K Gaming Monitor': 'Electronics',\n",
    "    'LG Dryer': 'Electronics',\n",
    "    'USB-C Charging Cable': 'Accessories',\n",
    "    'AA Batteries (4-pack)': 'Accessories',\n",
    "    'Bose SoundSport Headphones': 'Accessories',\n",
    "    'AAA Batteries (4-pack)': 'Accessories',\n",
    "    'Lightning Charging Cable': 'Accessories',\n",
    "    'Wired Headphones': 'Accessories',\n",
    "    'Flatscreen TV': 'Electronics',\n",
    "    'Product': 'Others',\n",
    "    None: 'Others'\n",
    "}\n",
    "\n",
    "# Create a new column 'Product Category' by mapping the 'Product' column using the dictionary\n",
    "sales_data['Product Category'] = sales_data['Product'].map(product_category_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics for Concatenated dataset\n",
    "sales_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Duplicate Rows\n",
    "sales_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completely_empty_rows = dap_dec.isnull().all(axis=1)\n",
    "\n",
    "# Step 2: Sum the boolean values indicating completely empty rows\n",
    "sum_completely_empty_rows = completely_empty_rows.sum()\n",
    "\n",
    "# Step 3: Print the total number of completely empty rows\n",
    "print(\"Total completely empty rows:\", sum_completely_empty_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any missing values\n",
    "sales_data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop customer ID.\n",
    "del sales_data['Order ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save concat sales_data to CSV for visualisation\n",
    "sales_data.to_csv('sales_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. <b> Univariate Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the distribution of Numerical Variables\n",
    "sales_data.hist(figsize=(12,8), grid= False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The most frequent quantity ordered 1.\n",
    "2. The most frequent price of each product is between 0-200\n",
    "\n",
    "In general, there seems to be no correlation between the quantity ordered and the price per order. There are expensive orders for small quantities and cheap orders for large quantities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single boxplot to show outliers\n",
    "cols = [ 'Quantity Ordered', 'Price Each']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.boxplot(data=sales_data[cols], whis=1.5, orient='h', palette=['skyblue', 'limegreen'])\n",
    "plt.title(\"Boxplot of Numerical Features with Outliers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot KDEs(kernel density estimation) for all columns\n",
    "fig, axes = plt.subplots(nrows=len(cols), figsize=(12, 20))\n",
    "for i, col in enumerate(cols):\n",
    "    sns.kdeplot(data=sales_data, x=col, ax=axes[i], fill=True)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Density')\n",
    "   \n",
    "    # Calculate mean, skewness, and kurtosis\n",
    "    mean_val = sales_data[col].mean()\n",
    "    skewness_val = sales_data[col].skew()\n",
    "    kurtosis_val = sales_data[col].kurtosis()\n",
    "   \n",
    "    # Add mean, skewness, and kurtosis as text annotations\n",
    "    axes[i].text(0.6, 0.9, f'Mean: {mean_val:.2f}', transform=axes[i].transAxes)\n",
    "    axes[i].text(0.6, 0.8, f'Skewness: {skewness_val:.2f}', transform=axes[i].transAxes)\n",
    "    axes[i].text(0.6, 0.7, f'Kurtosis: {kurtosis_val:.2f}', transform=axes[i].transAxes)\n",
    "   \n",
    "    # Add mean line\n",
    "    axes[i].axvline(mean_val, color='red', linestyle='--', label='Mean')\n",
    "   \n",
    "    # Add red dots to indicate potential outliers\n",
    "    outliers = sales_data[(sales_data[col] > mean_val + 3 * sales_data[col].std()) | (sales_data[col] < mean_val - 3 * sales_data[col].std())]\n",
    "    axes[i].plot(outliers[col], [0] * len(outliers), 'ro', label='Potential Outliers')\n",
    "   \n",
    "    # Add legend\n",
    "    axes[i].legend()\n",
    "   \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observations:\n",
    "1. the quantity ordered has a skweness of 4.83, so it suggests a positive skewness.The distribution  is heavily skewed towards higher values, with a long tail on the right side.\n",
    "2. The price data exhibits significant positive skewness and high kurtosis, indicating a distribution that is highly right-skewed with heavy tails and a sharp peak around the mean price. This suggests that while most prices are clustered around the mean, there are notable outliers or extreme values at both ends of the price spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. <b> Bivariate Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_quantity = sales_data.groupby('Product')['Quantity Ordered'].sum().sort_values(ascending=False)\n",
    "product_quantity.plot(kind='barh', figsize=(10, 6))  # Changing the plot type to 'barh' for horizontal bar plot\n",
    "plt.title('Total Quantity Ordered for Each Product')\n",
    "plt.xlabel('Total Quantity Ordered')\n",
    "plt.ylabel('Product')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From this chart, we can observe that the AAA Batteries (4-pack) are the most ordered product, with a total quantity that appears to be just shy of 30,000. The next most ordered product is the AA Batteries (4-pack), followed by the USB-C Charging Cable and the Lightning Charging Cable. These items are typically consumable or commonly used accessories, which may explain their higher order quantities.\n",
    "\n",
    "On the other end of the spectrum, the least ordered products are the LG Dryer and the LG Washing Machine. This could be due to several factors, such as higher prices, lower purchase frequency, or less demand compared to smaller electronic accessories.\n",
    "\n",
    "A notable observation from the mid-range of the product order quantities is that personal electronics like the MacBook Pro Laptop, ThinkPad Laptop, and various types of phones and monitors have a moderate level of orders, indicating a steady but lower demand compared to the cheaper, more consumable products.\n",
    "\n",
    "Overall, the chart indicates a trend where lower-cost and frequently used items have higher order volumes, while more expensive and durable goods have lower order volumes. This pattern is typical in consumer electronics markets where small accessories are purchased more frequently than large appliances or high-end electronics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product and Price Each\n",
    "product_price = sales_data.groupby('Product')['Price Each'].mean().sort_values(ascending=False)\n",
    "product_price.plot(kind='barh', figsize=(10, 6))\n",
    "plt.title('Average Price for Each Product')\n",
    "plt.xlabel('Product')\n",
    "plt.ylabel('Average Price')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few observations from the chart above:\n",
    "\n",
    "Product Price Range: There is a wide range of average prices among the products displayed. The lowest average price products are the AAA and AA batteries, while the highest average price is attributed to the MacBook Pro Laptop.\n",
    "\n",
    "Grouping by Price: The products seem to group into distinct categories based on their average price. Small accessories like batteries and charging cables are at the lower end, personal audio and monitors are in the middle range, while major appliances and high-end electronics like the MacBook Pro and iPhone are at the higher end.\n",
    "\n",
    "Variability in Price: There is significant variability in the average price of products, indicating a diverse set of items with different value propositions and target markets. For instance, the average price of a MacBook Pro Laptop is substantially higher than any other product on the list, which suggests it is likely a high-end product with premium features and branding.\n",
    "\n",
    "Comparison of Similar Products: Within similar product categories, there is a range of average prices that could reflect different brand values, features, or specifications. For example, the Apple Airpods Headphones have a higher average price compared to other audio products like the Bose SoundSport Headphones or Wired Headphones, indicating brand value or feature differences.\n",
    "\n",
    "Tech Product Categories: The graph covers a broad spectrum of tech product categories from simple consumables to complex electronics, which might be used to analyze market trends, consumer spending, or to inform stocking decisions for retailers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hexbin plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hexbin(sales_data['Price Each'], sales_data['Quantity Ordered'], gridsize=20, cmap='YlGnBu', edgecolors='k')\n",
    "plt.colorbar(label='count in bin')\n",
    "plt.title('Hexbin Plot of Quantity Ordered vs. Price Each')\n",
    "plt.xlabel('Price Each')\n",
    "plt.ylabel('Quantity Ordered')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of Orders:\n",
    "The majority of orders fall into the lower price range, as indicated by the concentration of darker hexbins in the lower left corner.\n",
    "These darker hexbins represent higher counts of orders for items with lower prices.\n",
    "Price and Quantity Relationship:\n",
    "As the price each decreases, there is a trend of increased quantity ordered.\n",
    "This suggests that customers are more likely to order larger quantities when the price per item is lower.\n",
    "Sparse Areas:\n",
    "Most hexbins are light-colored, indicating low counts.\n",
    "There are fewer orders for items with higher prices.\n",
    "Overall Trend:\n",
    "The graph shows that quantity ordered tends to be higher for lower-priced items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical columns\n",
    "numerical_columns = sales_data.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = numerical_columns.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heat map represents a correlation matrix between two variables: “Quantity Ordered” (on the y-axis) and “Price Each” (on the x-axis).\n",
    "The color scale ranges from blue (negative correlation) to red (positive correlation).\n",
    "\n",
    "Key observations:\n",
    "There is a perfect positive correlation of 1.00 between “Quantity Ordered” and itself (indicated by the red square in the top left corner).\n",
    "Similarly, there is a perfect positive correlation of 1.00 between “Price Each” and itself (shown by the red square in the bottom right corner).\n",
    "However, there is a weak negative correlation of approximately -0.15 between “Quantity Ordered” and “Price Each” (represented by the blue square in the top right corner).\n",
    "\n",
    "Interpretation:\n",
    "When “Quantity Ordered” increases, there is a corresponding increase in “Price Each” (positive correlation).\n",
    "The weak negative correlation suggests that as “Quantity Ordered” goes up, there is a slight decrease in the unit price (“Price Each”).\n",
    "\n",
    "Practical Implications:\n",
    "Businesses can use this information to optimize pricing strategies.\n",
    "Understanding the relationship between quantity and price helps in decision-making, inventory management, and revenue forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "sns.pairplot(sales_data, height=4)\n",
    "plt.suptitle('Pairplot of Numerical Variables', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairplot Overview:\n",
    "\n",
    "The plot displays a pairplot of two numerical variables: “Quantity Ordered” and “Price Each.”\n",
    "The diagonal contains histograms for each variable, while the off-diagonal cells show scatter plots.\n",
    "The x-axis labels represent “Quantity Ordered,” and the y-axis labels represent “Price Each.”\n",
    "\n",
    "Histograms:\n",
    "\n",
    "Quantity Ordered: The histogram for quantity ordered shows a significant spike at lower quantities. Most orders appear to be for small quantities.\n",
    "Price Each: The histogram for price each does not reveal a clear pattern or trend. Prices vary widely.\n",
    "\n",
    "Scatter Plots:\n",
    "\n",
    "There is no strong linear correlation between “Quantity Ordered” and “Price Each.” The scatter plots do not exhibit any distinct trend.\n",
    "The data points are scattered across the plot, indicating that the relationship between these variables is not straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **The Hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Null Hypothesis:** There is no difference in revenue generated between different product categories.\n",
    "  \n",
    "  **Alternative Hypothesis:** Certain product categories generate significantly more revenue compared to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate revenue\n",
    "sales_data['Revenue'] = sales_data['Quantity Ordered'] * sales_data['Price Each']\n",
    "\n",
    "# Extract revenue for each product category\n",
    "electronics_revenue = sales_data[sales_data['Product Category'] == 'Electronics']['Revenue']\n",
    "accessories_revenue = sales_data[sales_data['Product Category'] == 'Accessories']['Revenue']\n",
    "others_revenue = sales_data[sales_data['Product Category'] == 'Others']['Revenue']\n",
    "\n",
    "# Perform ANOVA test\n",
    "statistic, p_value = f_oneway(electronics_revenue, accessories_revenue, others_revenue)\n",
    "\n",
    "# Define significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Interpret the result\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis.\")\n",
    "    print(\"Certain product categories generate significantly more revenue compared to others.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis.\")\n",
    "    print(\"There is no difference in revenue generated between different product categories.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Analytical Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How much money did we make this year? \n",
    "\n",
    "2. Can we identify any seasonality in the  sales? \n",
    "\n",
    "3. What are our best and worst-selling products? \n",
    "\n",
    "4. How do sales compare to previous months or weeks? \n",
    "\n",
    "5. Which cities are our products delivered to most? \n",
    "\n",
    "6. How do product categories compare in revenue generated and quantities  ordered? \n",
    "\n",
    "7. What are the best and worst-selling products?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How much money did we make this year? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate revenue for each transaction\n",
    "sales_data['Revenue'] = sales_data['Quantity Ordered'] * sales_data['Price Each']\n",
    "\n",
    "# Sum up the revenue column to calculate total revenue\n",
    "total_revenue = sales_data['Revenue'].sum()\n",
    "\n",
    "# Print the total revenue\n",
    "print(\"Total revenue generated this year:\", total_revenue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data available, the total revenue generated this year amounts to $34.57 million. This substantial revenue reflects the combined impact of both the “Accessories” and “Electronics” product categories. It’s essential to continue monitoring these trends to inform strategic decisions and optimize sales performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Can we identify any seasonality in the  sales? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data_2019 = sales_data[sales_data['Order Date'].dt.year == 2019].copy()\n",
    "\n",
    "# Extract the month from the 'Order Date' column and assign it to a new column 'Order Month'\n",
    "sales_data_2019.loc[:, 'Order Month'] = sales_data_2019['Order Date'].dt.month\n",
    "\n",
    "# Group the sales data by month and sum the sales\n",
    "monthly_sales_2019 = sales_data_2019.groupby('Order Month')['Quantity Ordered'].sum()\n",
    "\n",
    "# Reindex the monthly_sales_2019 DataFrame to include all 12 months\n",
    "monthly_sales_2019 = monthly_sales_2019.reindex(range(1, 13), fill_value=0)\n",
    "\n",
    "\n",
    "# Plot the monthly sales data for the year 2019\n",
    "monthly_sales_2019.plot(kind='line', figsize=(10, 6), marker='o', color='b', linestyle='-')\n",
    "plt.title('Monthly Sales in 2019')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Quantity Ordered')\n",
    "plt.xticks(monthly_sales_2019.index, ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Seasonality Overview**:\n",
    "   - The line graph titled \"Monthly Sales in 2019\" reveals patterns that suggest **seasonal fluctuations**.\n",
    "   - Sales exhibit recurring cycles throughout the year.\n",
    "\n",
    "2. **Monthly Trends**:\n",
    "   - **March**: Sales peak at over 20,000.\n",
    "   - **May and June**: There is a noticeable dip, reaching its lowest point at approximately 12,500.\n",
    "   - **July to December**: Sales gradually increase, with another significant peak in December (approximately 27,500).\n",
    "\n",
    "3. **Interpretation**:\n",
    "   - The upward trend from July to December aligns with the **holiday season**, indicating strong sales during this period.\n",
    "   - The cyclic behavior suggests that sales are influenced by **seasonal factors**.\n",
    "\n",
    "In summary, the sales data exhibits seasonality, with notable peaks around March and December. Understanding these patterns can inform business strategies and forecasting efforts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What are our best and worst-selling products?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the sales data by product and sum the quantity ordered for each product\n",
    "product_sales = sales_data.groupby('Product')['Quantity Ordered'].sum()\n",
    "\n",
    "# Sort the products based on total quantity ordered in descending order\n",
    "product_sales_sorted = product_sales.sort_values(ascending=False)\n",
    "\n",
    "# Best-selling product (product with the highest quantity ordered)\n",
    "best_selling_product = product_sales_sorted.index[0]\n",
    "\n",
    "# Worst-selling product (product with the lowest quantity ordered)\n",
    "worst_selling_product = product_sales_sorted.index[-1]\n",
    "\n",
    "print(\"Best-selling product:\", best_selling_product)\n",
    "print(\"Worst-selling product:\", worst_selling_product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main factor that can be attributed to the observation above is:\n",
    "\n",
    "**Utility and Frequency of Use**: AAA batteries are commonly used in many household items such as remote controls, toys, flashlights, etc. They are consumable goods, meaning people need to regularly replace them. On the other hand, a dryer is a large appliance that people typically purchase infrequently and expect to last for several years without needing replacement.\n",
    "\n",
    "Other factors include:\n",
    "\n",
    "**Seasonal Variations**: The demand for AAA batteries might be relatively consistent throughout the year, whereas the demand for dryers could fluctuate based on factors like seasons (e.g., more people buying dryers during colder, wetter months).\n",
    "\n",
    "**Price Sensitivity and Disposable Income**: AAA batteries are generally inexpensive and considered essential items for many households, making them an easy and frequent purchase. Dryers, on the other hand, are larger investments that require more consideration and planning, especially if they're high-end models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the quantity ordered for each product\n",
    "plt.figure(figsize=(10, 6))\n",
    "product_sales_sorted.plot(kind='barh')\n",
    "plt.title('Quantity Ordered for Each Product')\n",
    "plt.xlabel('Product')\n",
    "plt.ylabel('Total Quantity Ordered')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How do sales compare to previous months or weeks? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for the year 2019\n",
    "sales_data_2019 = sales_data[sales_data['Order Date'].dt.year == 2019].copy()\n",
    "\n",
    "# Calculate revenue for each transaction\n",
    "sales_data_2019['Revenue'] = sales_data_2019['Quantity Ordered'] * sales_data_2019['Price Each']\n",
    "\n",
    "# Group by month and sum up the revenue for each month\n",
    "monthly_revenue = sales_data_2019.groupby(sales_data_2019['Order Date'].dt.to_period('M'))['Revenue'].sum()\n",
    "\n",
    "# Reset index to convert the grouped data back to a DataFrame\n",
    "monthly_revenue = monthly_revenue.reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(monthly_revenue['Order Date'].astype(str), monthly_revenue['Revenue'], marker='o', linestyle='-')\n",
    "plt.title('Monthly Revenue Trend in 2019')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Revenue')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comparing sales to previous months:\n",
    "\n",
    "January to July Decline:\n",
    "From January to July, sales gradually decreased.\n",
    "The lowest point was in July (around 2.5 million in revenue).\n",
    "\n",
    "August to October Stability:\n",
    "\n",
    "In August, there was a slight increase in sales.\n",
    "Sales remained relatively stable from August to October.\n",
    "\n",
    "November and December Surge:\n",
    "\n",
    "The most significant change occurred from October to December.\n",
    "Sales skyrocketed, reaching their peak in December (close to 4.5 million in revenue).\n",
    "\n",
    "In summary, sales experienced a seasonal surge towards the end of the year, likely influenced by holiday shopping. The upward trend in November and December suggests successful year-end sales efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Which cities are our products delivered to most? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract city information from 'Purchase Address'\n",
    "sales_data['City'] = sales_data['Purchase Address'].apply(lambda x: x.split(',')[1])\n",
    "\n",
    "# Count the occurrences of each city\n",
    "city_counts = sales_data['City'].value_counts()\n",
    "\n",
    "# Now you can identify the cities where our products are delivered to most\n",
    "print(\"Top 5 cities where our products are delivered to most:\")\n",
    "print(city_counts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation regarding product deliveries indicates that San Francisco has the highest volume of deliveries, with a count of 20,506. Following San Francisco, Los Angeles and New York City also exhibit substantial delivery volumes, with 13,665 and 11,357 deliveries, respectively. Additionally, Boston and Atlanta demonstrate significant delivery numbers, with 9,044 and 6,868 deliveries, respectively. These figures suggest a concentration of product distribution in major urban centers, particularly in San Francisco, Los Angeles, and New York City. Understanding these delivery patterns can help in strategic planning, resource allocation, and targeted marketing efforts to maximize sales potential in these key metropolitan areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the top cities for 2019\n",
    "plt.figure(figsize=(10, 6))\n",
    "city_counts.head().plot(kind='bar', color='skyblue')\n",
    "plt.title('Top Cities for Product Deliveries in 2019')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Number of Deliveries')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How do product categories compare in revenue generated and quantities  ordered? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the sales data by product category and calculate total revenue and quantity ordered\n",
    "category_comparison = sales_data.groupby('Product Category').agg({'Revenue': 'sum', 'Quantity Ordered': 'sum'})\n",
    "\n",
    "# Plot the comparison\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Revenue comparison\n",
    "ax[0].bar(category_comparison.index, category_comparison['Revenue'], color='skyblue')\n",
    "ax[0].set_title('Revenue Generated by Product Category')\n",
    "ax[0].set_xlabel('Product Category')\n",
    "ax[0].set_ylabel('Total Revenue')\n",
    "\n",
    "# Quantity ordered comparison\n",
    "ax[1].bar(category_comparison.index, category_comparison['Quantity Ordered'], color='salmon')\n",
    "ax[1].set_title('Quantities Ordered by Product Category')\n",
    "ax[1].set_xlabel('Product Category')\n",
    "ax[1].set_ylabel('Total Quantity Ordered')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Revenue by Product Category:\n",
    "The graph on the left shows the revenue generated by two product categories: “Accessories” and “Electronics.”\n",
    "The “Electronics” category has significantly higher revenue compared to “Accessories.”\n",
    "This suggests that electronics products contribute more to overall revenue.\n",
    "2. Quantities Ordered by Product Category:\n",
    "The graph on the right displays the quantities ordered for the same product categories.\n",
    "Interestingly, “Accessories” have a higher quantity ordered compared to “Electronics.”\n",
    "Despite lower revenue, accessories seem to be more popular in terms of the number of orders.\n",
    "In summary, while electronics generate substantial revenue, accessories attract more customers in terms of quantity ordered. This information could be valuable for decision-making and business strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Are there any price ranges that attract more customers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sales_data['Price Each'], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Price Ranges vs Quantity of Orders')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Quantity of Orders')\n",
    "plt.grid(False)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Distribution of Orders by Price Range:\n",
    "The majority of orders fall within the 0-250 price range, as indicated by the very tall bar. Nearly 100,000 orders were placed in this range.\n",
    "As the price range increases, the quantity of orders decreases significantly.\n",
    "2. Decreasing Order Volume with Higher Prices:\n",
    "The shorter bars represent higher price ranges (250-500, 500-750, etc.). These ranges have significantly fewer orders.\n",
    "The trend suggests that customers are more likely to make purchases for products priced below 250 dollars.\n",
    "3. Business Implications:\n",
    "To drive sales, consider focusing on products within the lower price range, as they attract the highest order volume.\n",
    "For higher-priced items, targeted marketing efforts or promotions may be necessary to boost sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Product Deliveries by City:\n",
    "San Francisco leads with the highest volume of deliveries (20,506).\n",
    "Los Angeles and New York City follow closely with 13,665 and 11,357 deliveries, respectively.\n",
    "Boston and Atlanta also demonstrate significant delivery numbers (9,044 and 6,868, respectively).\n",
    "Concentration in major urban centers (San Francisco, Los Angeles, and New York City) suggests strategic planning opportunities for resource allocation and targeted marketing efforts\n",
    "2. Revenue by Product Category:\n",
    "The graph on the left shows revenue generated by two product categories: “Accessories” and “Electronics.”\n",
    "“Electronics” significantly outperforms “Accessories” in terms of revenue.\n",
    "This indicates that electronics products contribute more to overall revenue.\n",
    "3. Quantities Ordered by Product Category:\n",
    "The graph on the right displays quantities ordered for the same product categories.\n",
    "Interestingly, “Accessories” have a higher quantity ordered compared to “Electronics.”\n",
    "Despite lower revenue, accessories seem to be more popular in terms of the number of orders.\n",
    "4. Distribution of Orders by Price Range:\n",
    "The majority of orders fall within the 0-250 price range, as indicated by the very tall bar. Nearly 100,000 orders were placed in this range.\n",
    "As the price range increases, the quantity of orders decreases significantly.\n",
    "The shorter bars represent higher price ranges (250-500, 500-750, etc.), which have significantly fewer orders.\n",
    "\n",
    "In conclusion, while electronics generate substantial revenue, accessories attract more customers in terms of quantity ordered. Understanding these patterns can inform decision-making and business strategy. 📊💡\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
